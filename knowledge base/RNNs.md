[Wiki article](https://en.wikipedia.org/wiki/Recurrent_neural_network)

Sourced from Default GPT-3.5:

>An RNN (Recurrent Neural Network) is a type of neural network that is designed to process sequential data, such as time series or natural language text. Unlike feedforward neural networks, which process each input independently and produce an output, RNNs have a memory component that allows them to maintain information about previous inputs and use it to influence the current output.
>
>The key idea behind RNNs is that they use feedback loops to connect previous inputs with current outputs. The output from the previous time step is used as an input to the current time step, allowing the network to maintain an internal state or memory of the previous inputs.
>
>RNNs are useful in a wide range of applications, including speech recognition, machine translation, image captioning, and sentiment analysis. One of the most popular variants of RNNs is the LSTM (Long Short-Term Memory) network, which is designed to overcome the problem of vanishing gradients that can occur when training deep neural networks.

**I should watch some youtube videos about this**


