[Prev](/B-parking/Mon_Apr_10_2023)

I should make a set of slides today for the meeting tomorrow morning at 8 am.

Ok, let's build a narrative here.

Originally we were wondering why the jan 2023 plot see such a radical improvement from this plot here, from october 2022:

![oct2022plots](/img/oct2022plots.png)
^oct2022plots

to the left plot given in the jan 2023 EXO talk:
![jan2023plots](/img/jan2023plots.png)
^jan2023plots

Let's ignore for now that the [jan 2023 plots](#^jan2023plots) and [oct 2022 plot](#^oct2022plots) have different samples for now. The reason for this can be found in [these notes](/B-parking/Mon_Apr_10_2023).

Both plots in the [jan 2023 plots](#^jan2023plots) come from the same sample and background set, from Aram's running of the BigNtupler analyzer:
```python
prefix = 'root://cmseos.fnal.gov//store/user/ahayrape/BigNtupler/'

fileset = {
			'PhiToPi0Pi0_mPhi0p3_ctau300':
				[prefix + 'PhiToPi0Pi0_mPhi0p3_ctau300.root'],
				
			'PhiToPi0Pi0_mPhi1p0_ctau1000':
				[prefix + 'PhiToPi0Pi0_mPhi1p0_ctau1000.root'],
				
			'PhiToPiPlusPiMinus_mPhi0p3_ctau300':
				[prefix + 'PhiToPiPlusPiMinus_mPhi0p3_ctau300.root'],
				
			'PhiToPiPlusPiMinus_mPhi1p0_ctau1000':
				[prefix + 'PhiToPiPlusPiMinus_mPhi1p0_ctau1000.root'],
				
			'PhiToPiPlusPiMinus_mPhi1p0_ctau300':
				[prefix + 'PhiToPiPlusPiMinus_mPhi1p0_ctau300.root'],
		 }

fileset['background'] = [prefix + 'Background.root']
```

 However, the plot in the [oct 2022 plot](#^oct2022plots) comes from Christina's "v9" version of samples.

```python
prefix = '/eos/uscms/store/user/christiw/bparking/V1p19/MC_Fall18/v1/v9/normalized/'

filenames = [
    'BToKPhi_MuonGenFilter_PhiToPi0Pi0_mPhi0p3_ctau1000_1pb_weighted.root',
    'BToKPhi_MuonGenFilter_PhiToPi0Pi0_mPhi0p3_ctau300_1pb_weighted.root',
    'BToKPhi_MuonGenFilter_PhiToPiPlusPiMinus_mPhi0p3_ctau1000_1pb_weighted.root',
    'BToKPhi_MuonGenFilter_PhiToPiPlusPiMinus_mPhi0p3_ctau300_1pb_weighted.root',
]

fileset = {
    f"{file.split('_')[2]}_{file.split('_')[3]}_{file.split('_')[4]}": 
    [prefix+file] 
    for file in filenames
}

fileset['background'] = ['/eos/uscms/store/user/christiw/bparking/V1p19/Data2018_UL/v9/normalized/ParkingBPH4_2018A_goodLumi.root']
```

There are a few main differences between these sets.
1. Aram's has SF calculated while Christina's does not
2. Christina's has only a muon filter applied while Aram's has both muon and LLP filters applied
3. Aram's samples have more events while Christina's background has more events
4. We know how much of the b-parking set Christina's background was run on, ~2%, while we don't know how much Aram's is run off of.

As we will see later, 2 and 4 turn out to be problematic.

If we continue to do studies over multiple version of the samples, I think it would be best to make a "genealogy" of all of them, with descriptions on how they're different. This has been the harder part of this study.

For now let's just look at the [jan 2023 plots](#^jan2023plots) and see if there's something wrong here. 

The major difference between these two BR limits is how their yields are calculated. 

Note to self that in this pseudo code, some dictionary slicing is removed for clarity.

On the left (old): ^d1dd8c
```python
dem_cut = 'pretrigger'

for i, new_ctau in enumerate(ctau_list):
	weight_terms = (old_ctau/new_ctau) * np.exp(gLLP_ctau*(1/old_ctau - 1/new_ctau))
		
	weight = np.sum(weight_terms)
	
	new_nEvents[i] = weight

N_sig = N_bb * new_nEvents / nEvents[dem_cut]

N_bkg = N_bb_p * N_bb * nEvents['background'][num_cut]/nEvents['background'][dem_cut]
```

On the right (new): ^18eb94
```python
dem_cut = 'pretrigger'

for i, new_ctau in enumerate(ctau_list):
	weight_terms = (old_ctau/new_ctau) * np.exp(gLLP_ctau*(1/old_ctau - 1/new_ctau))
	
	weight_terms *= muon_SF
	
	weight = np.sum(weight_terms)
	
	new_nEvents[i] = weight

N_sig = muonfiltereff * genfiltereff * new_nEvents * xsec * lumi / nEvents[dem_cut]

N_bkg = N_bb_p * N_bb * nEvents['background'][num_cut]/nEvents['background'][dem_cut]
```

#### Differences in `N_sig`

To point out the key differences, [***old***](#^d1dd8c) uses an estimate of the number of events in the B-parking dataset (`N_bb`) ==**instead of**== the luminosity (`lumi`), cross-section (`xsec`), and scale factor (`muon_SF`) to estimate the yield.

Whereas [***new***](#^18eb94) uses an estimate of the number of events in the B-parking dataset using the following formula: `N_bb` ~ `muonfiltereff * xsec * lumi * muon_SF`. Additionally we add a factor of the LLP efficiency (`genfiltereff`) to account for the fact that we are using an LLP filtered sample set.

**Here's our first problem:** Since ==both BR limits, [***old***](#^d1dd8c)  and [***new***](#^18eb94), use the same LLP filtered samples==, they ==both have to take the LLP gen filter efficiency into account==. This is ==not done for [***old***](#^d1dd8c) ==. This needs to be done for [***old***](#^d1dd8c) otherwise Aram's samples are not comparable to Christina's samples.

#### Differences in N_bkg

There are no differences, and that's an issue. The variable `N_bb_p` keeps track of how much of the B-parking dataset was skimmed to make the "background" datasets. For Christina we know it to be about 2%. We don't know what it is for Aram's. 

**So that leads us to our second problem:** We are ==using the wrong `N_bb_p` for Aram's sample set==. The way we solve this is by using Christina's background instead of Aram's

So to recap, the changes that need to be made are:
* apply the same LLP filter efficiency to both calculations of `N_sig`
* use Christina's background instead of Aram's when estimating the `N_bkg`

So here are the resulting plots:
![](/img/6971833B-3BDE-4CE8-B96C-131BFA3D71C1.png)

And so both limits are worse, but at least the left plot is consistent with the [oct 2022 plot](#^oct2022plots).

The total improvement is around 30-45x, which seems too good to be true still?
![](/img/30D4883B-0963-4CB1-A63C-907A35FB1D44.png)


 