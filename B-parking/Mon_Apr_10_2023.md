[Prev](/B-parking/Fri_Apr_7_2023.md)

Plan for today

wait for aram's response on how much data he ran the bkg using the bigntupler
one i get this i can rerun the limits and we'll see how that goes

I should also try running christinas v9 bkg on aram's newer cutflow code

then after, move onto the pileup weight stuff
- we need to get pilup dist from data and signal
- using pileup we need to reweight the yield in sig to data
- remind christina to send instrucrtions 

Ok here we go.

In the past I used two different sample sets and two different data sets.

| **Christina's v9** | **from Aram's bigntupler** |
| --------                            | --------                            | 
| PhiToPi0Pi0_mPhi0p3_ctau1000        |                                     |
| ✅ PhiToPi0Pi0_mPhi0p3_ctau300         | ✅ PhiToPi0Pi0_mPhi0p3_ctau300         |
| PhiToPiPlusPiMinus_mPhi0p3_ctau1000 |                                     |
| ✅ PhiToPiPlusPiMinus_mPhi0p3_ctau30   | ✅ PhiToPiPlusPiMinus_mPhi0p3_ctau30   |
|                                     | PhiToPi0Pi0_mPhi1p0_ctau1000        |
|                                     | PhiToPiPlusPiMinus_mPhi1p0_ctau1000 |
|                                     | PhiToPiPlusPiMinus_mPhi1p0_ctau300  |
| ✅ Background                          | ✅ Background |

^aebbf6

Those marked with a check (✅) are the only ones comparable between the sets.

A problem we came up against last time was realizing that I make the mistake of assuming that the amount of bkg run through the ntupler is the same for both sample sets. 
This is a mistake.

We know for Christina's v9 bkg, it was around 2% of data. That is not necessarily the same for Aram's bkg. We don't actually know how much data he ran off of, and it turns out he maybe performed cuts in the analyzer that would skew out calculation of the BR from the cutflow code. So as a remedy, I ought to calculate the BR limits using Christina's bkg  run on the cutflow code instead of Aram's bkg.

In short, these are the components I need to put together:

Samples:
* Aram's
	* PhiToPiPlusPiMinus_mPhi0p3_ctau30
	* PhiToPi0Pi0_mPhi0p3_ctau300

Background:
* Chrstina's

Cutflow:
* Aram's cutflow code

BR limits:
* my own BR limit code

We seem to get much more comparable BR limits after this procedure:
![mon_apr_10_2023_plot1](B-parking/img/mon_apr_10_2023_plot1.png)

With a factor difference of only about 1-2
![mon_apr_10_2023_plot2](B-parking/img/mon_apr_10_2023_plot2.png)

Apparantly, I am getting word that I should not have been applying the muon filter eff....

Ok so I will try that now as well as applying xsec and lumi to Aram's, as well as the SF.

In short, if I am applying the filter efficiencies, I do not apply N_bb.

This is confusing, but apparently the following holds
```python
N_bb ~ muonfiltereff * genfiltereff * xsec * lumi * SF
```

So just to be clear here, the code used to calculate the BR for their samples are:

**Aram's**
```python
muon_SF = np.array(muon_SFs[dataset][num_cut])
for i, new_ctau in enumerate(ctau_list):
	weight_terms = (old_ctau/new_ctau) * np.exp(gLLP_ctau*(1/old_ctau - 1/new_ctau))
	weight_terms *= muon_SF 
	weight = np.sum(weight_terms)
	new_nEvents[i] = weight

N_sig = muonfiltereff * genfiltereff[dataset] * new_nEvents * xsec * lumi / nEvents[dataset][dem_cut]
		
N_bkg = N_bb_p * N_bb * nEvents['background'][num_cut]/nEvents['background'][dem_cut] 

BR = 2 * np.sqrt(N_bkg)/N_sig
```

**Christina's**
```python
for i, new_ctau in enumerate(ctau_list2):
	weight_terms = (old_ctau/new_ctau) * np.exp(gLLP_ctau2*(1/old_ctau - 1/new_ctau))
	weight = np.sum(weight_terms)
	new_nEvents2[i] = weight
						
N_sig2 = N_bb * new_nEvents2 / nEvents2[dataset][dem_cut]

N_bkg2 = N_bb_p * N_bb * nEvents2['background'][num_cut]/nEvents2['background'][dem_cut] 

BR2 = 2 * np.sqrt(N_bkg2)/N_sig2
```

Here are the results:
![mon_apr_10_2023_plot3](B-parking/img/mon_apr_10_2023_plot3.png)

With ratio:
![mon_apr_10_2023_plot4](B-parking/img/mon_apr_10_2023_plot4.png)